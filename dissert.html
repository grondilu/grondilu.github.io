<!DOCTYPE html>
<html>
    <head>
    <title>The Theory of the Universal Wave Function</title>
    <meta name="author" content="Hugh Everett, III">
    <meta charset="UTF-8">

    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({ TeX: { equationNumbers: {autoNumber: "all"} } });
    </script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
    </head>

    <body>

    <p>This is an HTML version of <a href=https://www-tc.pbs.org/wgbh/nova/manyworlds/pdf/dissertation.pdf>Hugh Everett's dissertation</a>.</p>

    <h1>I. Introduction</h1>

    <p>We begin, as a way of entering our subject, by characterizing a particular interpretation of quantum theory which, although not representative of the more careful formulations of some writers, is the most common form encountered in textbooks and university lectures on the subject.</p>

    <p>A physical system is described completely by a state function \(\varphi\), which is an element of a Hilbert Space, and which furthermore gives information only concerning the probabilities of the results of various observations which can be made on the system. The state function \(\varphi\) is thought of as objectively characterizing the physical system, i.e., at all times an isolated system is thought of as possessing a state function, independently of our state of knowledge of it. On the other hand, \(\varphi\) changes in a causal manner so long as the system remains isolated, obeying a differential equation. Thus there are two fundamentally different ways in which the state function can change<sup><a href="#fn1">1</a></sup>: </p>

    <dl>
    <dt>Process 1</dt>
    <dd>The discontinuous change brought about by the observation of a quantity with eigenstates \(\phi_1, \phi_2\, \ldots\), in which the state \(\varphi\) will be changed to the state \(\phi_j\) with probability \(|(\varphi,\phi_j)|^2\).</dd>
    <dt>Process 2</dt>
    <dd>The continuous, deterministic change of state of the (isolated) system with time according to a wave equation \(\frac{\partial}{\partial t}\varphi = U\varphi\), where \(U\) is a linear operator.</dd>
    </dl>

    <p>The question of the consistency of the scheme arises if one contemplates regarding the observer and his object-system as a single (composite) physical system. Indeed, the situation becomes quite paradoxical if we allow for the existence of more than one observer. Let us consider the case of one observer A, who is performing measurements upon a system S, the totality (A + S) in turn forming the object-system for another observer, B.</p>

    <p>If we are to deny the possibility of B's use of a quantum mechanical description (wave function obeying wave equation) for A + S, then we must be supplied with some alternative description for systems which contain observers (or measuring apparatus). Furthermore, we would have to have a criterion for telling precisely what type of systems would have the preferred positions of "measuring apparatus" or "observer" and be subject to the alternate description. Such a criterion is probably not capable of rigorous formulation.</p>

    <p>On the other hand, if we do allow B to give a quantum description to A + S, by assigning a state function \(\psi^{A+S}\), then, so long as B does not interact with A + S, its state changes causally according to Process 2, even though A may be performing measurements upon S. From B's point of view, nothing resembling Process 1 can occur (there are no discontinuities), and the question of the validity of A's use of Process 1 is raised.  That is, <em>apparently</em> either A is incorrect in assuming Process 1, with its probabilistic implications, to apply to his measurements, or else B's state function, with its purely causal character, is an inadequate description of what is happening to A + S.</p>

    <p>To better illustrate the paradoxes which can arise from strict adherence to this interpretation we consider the following amusing, but <em>extremely hypothetical drama</em>.</p>

    <blockquote>
    <p>Isolated somewhere out in space is a room containing an observer, A, who is about to perform a measurement upon a system S. After performing his measurement he will record the result in his notebook.  We assume that he knows the state function of S (perhaps as a result of previous measurement), and that it is not an eigenstate of the measurement he is about to perform. A, being an orthodox quantum theorist, then believes that the outcome of his measurement is undetermined and that the process is correctly described by Process 1.</p>
    <p>In the meantime, however, there is another observer, B, outside the room, who is in possession of the state function of the entire room, including S, the measuring apparatus, and A, just prior to the measurement.  B is only interested in what will be found in the notebook one week hence, so he computes the state function of the room for one week in the future according to Process 2. One week passes, and we find B still in possession of the state function of the room, which this equally orthodox quantum theorist believes to be a complete description of the room and its contents. If B's state function calculation tells beforehand exactly what is going to be in the notebook, then A is incorrect in his belief about the indeterminacy of the outcome of his measurement. We therefore assume that B's state function contains non-zero amplitudes over several of the notebook entries.</p>
    <p>At this point, B opens the door to the room and looks at the notebook (performs his observation). Having observed the notebook entry, he turns to A and informs him in a patronizing manner that since his (B's) wave function just prior to his entry into the room, which he knows to have been a complete description of the room and its contents, had non-zero amplitude over other than the present result of the measurement, the result must have been decided only when B entered the room, so that A, his notebook entry, and his memory about what occurred one week ago had no independent objective existence until the intervention by B. In short, B implies that A owes his present objective existence to B's generous nature which compelled him to intervene on his behalf. However, to B's consternation, A does not react with anything like the respect and gratitude he should exhibit towards B, and at the end of a somewhat heated reply, in which A conveys in a colorful manner his opinion of B and his beliefs, he rudely punctures B's ego by observing that if B's view is correct, then he has no reason to feel complacent, since the whole present situation may have no objective existence, but may depend upon the future actions of yet another observer.</p>
    </blockquote>

    <p>It is now clear that the interpretation of quantum mechanics with which we began is untenable if we are to consider. a universe containing more than one observer. We must therefore seek.a suitable modification of this scheme, or an entirely different system of interpretation. Several alternatives which avoid the paradox are:</p>

    <dl>
    <dt>Alternative 1</dt>
    <dd>To postulate the existence of only one observer in the universe. This is the solipsist position, in which each of us must hold the view that he alone is the only valid observer, with the rest of the universe and its inhabitants obeying at all times Process 2 except when under his observation.</dd>
    </dl>

    <p>This view is quite consistent, but one must feel uneasy when, for example, writing textbooks on quantum mechanics, describing Process 1, for the consumption of other persons to whom it does not apply.</p>

    <dl>
    <dt>Alternative 2</dt>
    <dd>To limit the applicability of quantum mechanics by asserting that the quantum mechanical description fails when applied to observers, or to measuring. apparatus, or more generally to systems approaching macroscopic size.</dd>
    </dl>

    <p>If we try to limit the applicability so as to exclude measuring apparatus, or in general systems of macroscopic size, we are faced with the difficulty of sharply defining the region of validity. For what n might a group of n particles be construed as forming a measuring device so that the quantum description fails? And to draw the line at human or animal observers, i.e., to assume that all mechanical aparata obey the usual laws, but that they are somehow not valid for living observers, does violence to the so-called principle of psycho-physical parallelism<sup><a href="#fn2">2</a></sup>, and constitutes a view to be avoided, if possible. To do justice to this principle we must insist that we be able to conceive of mechanical devices (such as servomechanisms), obeying natural laws, which we would be willing to call observers.</p>

    <dl>
    <dt>Alternative 3</dt>
    <dd>To admit the validity of the state function description, but to deny the possibility that B could ever be in possession of the state function' of A + S. Thus one might argue that a determination of the state of A would constitute such a drastic intervention that A would cease to function as an observer.</dd>
    </dl>

    <p>The first objection to this view is that no matter what the state of A + S is, there is in principle a complete set of commuting operators for which it is an eigenstate, so that, at least, the determination of these quantities will not affect the state nor in any way disrupt the operation of A. There are no fundamental restrictions in the usual theory about the knowability of any state functions, and the introduction of any such restrictions to avoid the paradox must therefore require extra postulates.</p> <p>The second objection is that it is not particularly relevant whether or not B actually knows the precise state function of A + S. If he merely believes that the system is described by a state function, which he does not presume to know, then the difficulty still exists. He must then believe that this state function changed deterministically, and hence that there was nothing probabilistic in A's determination.</p>

    <dl><dt>Alternative 4</dt>
    <dd>To abandon the position that the state function is a complete description of a system. The state function is to be regarded not as a description of a single system, but of an ensemble of systems, so that the probabilistic assertions arise naturally from the incompleteness of the description.</dd>
    </dl>

    <p>It is assumed that the correct complete description, which would presumably involve further (hidden) parameters beyond the state function alone, would lead to a deterministic theory, from which the probabilistic aspects arise as a result of our ignorance of these extra parameters in the same manner as in classical statistical mechanics.</p>

    <dl><dt>Alternative 5</dt>
    <dd>To assume the universal validity of the quantum description, by the complete abandonment of Process 1. The general validity of pure wave mechanics, without any statistical assertions, is assumed for all physical systems, including observers and measuring apparata. Observation processes are to be described completely by the state function of the composite system which includes the observer and his object-system, and which at all times obeys the wave equation (Process 2)</dd>
    </dl>

    <p>This brief list of alternatives is not meant to be exhaustive, but has been presented in the spirit of a preliminary orientation. We have, in fact, omitted one of the foremost interpretations of quantum theory, namely the position of Niels Bohr. The discussion will be resumed in the final chapter, when we shall be in a position to give a more adequate appraisal of the various alternate interpretations. For the present, however, we shall concern ourselves only with the development of Alternative 5.</p> <p> It is evident that Alternative 5 is a theory of many advantages. It has the virtue of logical simplicity and it is complete in the sense that it is applicable to the entire universe. All processes are considered equally (there are no "measurement processes" which play any preferred role), and the principle of psycho-physical parallelism is fully maintained. Since the universal validity of the state function description is asserted, one can regard the state functions themselves as the fundamental entities, and one can even consider the state function of the whole universe. In this sense this theory can be called the theory of the "universal wave function, " since all of physics is presumed to follow from this function alone. There remains, however, the question whether or not such a theory can be put into correspondence with our experience.</p>

    <p><em>The present thesis is devoted to showing that this concept of a universal wave mechanics, together with the necessary correlation machinery for its interpretation, forms a logically self consistent description of a universe in which several observers are at work.</em></p>

    <p>We shall be able to Introduce into the theory systems which represent observers. Such systems can be conceived as automatically functioning machines (servomechanisms) possessing recording devices (memory) and which are capable of responding to their environment. The behavior of these observers shall always be treated within the framework of wave mechanics. Furthermore, we shall deduce the probabilistic assertions of Process 1 as <em>subjective</em> appearances to such observers, thus placing the theory in correspondence with experience. We are then led to the novel situation in which the formal theory is objectively continuous and causal, while subjectively discontinuous and probabilistic. While this point of view thus shall ultimately justify our use of the statistical assertions of the orthodox view, it enables us to do so in a logically consistent manner, allowing for the existence of other observers. At the same time it gives a deeper insight into the meaning of quantized systems, and the role played by quantum mechanical correlations.</p>

    <p>In order to bring about this correspondence with experience for the pure wave mechanical theory, we shall exploit the correlation between subsystems of a composite system which is described by a state function.  A subsystem of such a composite system does not, in general, possess an independent state function. That is, in general a composite system cannot be represented by a single pair of subsystem states, but can be repre sented only by a superposition of such pairs of subsystem states. For example, the Schrodinger wave function for a pair of particles, \(\psi(x_1,x_2)\), cannot always be written in the form \(\psi = \phi(x_1)\eta(x_2)\), but only in the form \(\psi = \sum_{i,j} a_{ij}\phi^i(x_1)\eta^j(x_2)\).  In the latter case, there is no single state for Particle 1 alone or Particle 2 alone, but only the superposition of such cases.</p>

    <p>In fact, to any arbitrary choice of state for one subsystem there will correspond a <dfn>relative state</dfn> for the other subsystem, which will generally be dependent upon the choice of state for the first subsystem, so that the state of one subsystem is not independent, but correlated to the state of the remaining subsystem. Such correlations between systems arise from interaction of the systems, and from our point of view all measurement and observation processes are to be regarded simply as interactions between observer and object-system which produce strong correlations.</p>

    <p>Let one regard an observer as a subsystem of the composite system: observer + object-system. It is then an inescapable consequence that after the interaction has taken place there will not, generally, exist a single observer state. There will, however, be a superposition of the composite system states, each element of which contains a definite observer state and a definite relative object-system state. Furthermore, as we shall see, <em>each</em> of these relative object-system states will be, approximately, the eigenstates of the observation corresponding to the value obtained by the observer which is described by the same element of the superposition.  Thus, each element of the resulting superposition describes an observer who perceived. a definite and generally different result, and to whom it appears that the object-system state has been transformed into the corresponding eigenstate. In this sense the usual assertions of Process 1 appear to hold on a subjective level to each observer described by an element of the superposition. We shall also see that correlation plays an important role in preserving consistency when several observers are present and allowed to interact with one another (to "consult" one another) as well as with other object-systems.</p>

    <p>In order to develop a language for interpreting our pure wave mechanics for composite systems we shall find it useful to develop quantitative definitions for such notions as the "sharpness" or "definiteness" of an operator A for a state \(\psi\), and the "degree of correlation" between the subsystems of a.composite system or between a pair of operators in the subsystems, so that we can use these concepts in an unambiguous manner.  The mathematical development of these notions will be carried out in the next chapter (II) using some concepts borrowed from Information Theory <sup><a href="#fn3">3</a></sup>.  We shall develop there the general definitions of information and correlation, as well as some of their more important properties. Throughout Chapter II we shall use the language of probability theory to facilitate the exposition, and because it enables us to introduce in a unified manner a number of concepts that will be of later use. We shall nevertheless subsequently apply the mathematical definitions directly to state functions, by replacing probabilities by square amplitudes, <em>without, however, making any reference to probability models.</em></p>

    <p>Having set the stage, so to speak, with Chapter II, we turn to quantum mechanics in Chapter III. There we first investigate the quantum formalism of composite systems, particularly the concept of relative state functions, and the meaning of the representation of subsystems by non-interfering mixtures of states characterized by density matrices. The notions of information and correlation are then applied to quantum mechanics.  The final section of this chapter discusses the measurement process, which is regarded simply as a correlation-inducing interaction between subsystems of a single isolated system. A simple example of such a measurement is given and discussed, and some general consequences of the superposition principle are considered.</p>

    <p>This will be followed by an abstract treatment of the problem of Observation (Chapter IV). In this chapter we make use only of the superposition principle, and general rules by which composite system states are formed of subsystem states, in order that our results shall have the greatest generality and be applicable to any form of quantum theory for which these principles hold. (Elsewhere, when giving examples, we restrict ourselves to the non-relativistic Schrodinger Theory for simplicity.) The validity of Process 1 as a subjective phenomenon is deduced, as well as the consistency of allowing several observers to interact with one another.</p> <p>Chapter V supplements the abstract treatment of Chapter IV by discussing a number of diverse topics from the point of view of the theory of pure wave mechanics, including the existence and meaning of macroscopic objects in the light of their atomic constitution, amplification processes in measurement, questions of reversibility and irreversibility, and approximate measurement.  </p> <p>The final chapter summarizes the situation, and continues the discussion of alternate interpretations of quantum mechanics.</p>

    <h1>II. Probability, Information, and Correlation</h1>

    <p>The present chapter is devoted to the mathematical development of the concepts of information and correlation. As mentioned in the introduction we shall use the language of probability theory throughout this chapter to facilitate the exposition, although we shall apply the mathematical definitions and formulas in later chapters without reference to probability models.  We shall develop our definitions and theorems in full generality, for probability distributions over arbitrary sets, rather than merely for distributions over real numbers, with which we are mainly interested at present. We take this course because it is as easy as the restricted development, and because it gives a better insight into the subject.</p>

    <p>The first three sections develop definitions and properties of information and correlation for probability distributions over <em>finite</em> sets only. In section four the definition of correlation is extended to distributions over arbitrary sets, and the general invariance of the correlation is proved.  Section five then generalizes the definition of information to distributions over arbitrary sets. Finally, as illustrative examples, sections seven and eight give brief applications to stochastic processes and classical mechanics, respectively.</p>

    <h2>§1. Finite joint distributions</h2>

    <p>We assume that we have a collection of finite sets, \(\cal X, Y, \ldots Z\), whose elements are denoted by \(x_i\in\cal{X},\, y_j\in\cal{Y},\ldots\, z_k\in\cal{Z}\), etc., and that we have a <dfn>joint probability distribution</dfn>, \(P = P(x_i, y_j,\ldots, z_k)\), defined on the cartesian product of the sets, which represents the probability of the combined event \(x_i, y_j,\ldots,\) and \(z_k\).  We then denote by X,Y, ... ,Z the random variables whose values are the elements of the sets \(\cal X, Y, \ldots Z\), with probabilities given by P.  </p>

    <p>For any subset Y,...,Z of a set of random variables W,...,X,Y,...,Z, with joint probability distribution \(P(w_i,\ldots,x_j,y_k,\ldots,z_l)\), the <dfn>marginal distribution</dfn>, \(P(y_k,\ldots,z_l)\), is defined to be: $$\begin{equation} P(y_k,\ldots,z_l) = \sum_{i,\ldots,j}P(w_i,\ldots,x_j,y_k,\ldots,z_l) \label{eq:1.1} \end{equation}$$ which represents the probability of the joint occurrence of \(y_k,\ldots,z_l\) with no restrictions upon the remaining variables.</p>

    <p>For any subset Y, ... ,Z of a set of random variables the <dfn>conditional distribution</dfn>, conditioned upon the values \(W = w_i,\ldots,X=x_j\) for any remaining subset W,...,X and denoted by \(P^{w_i,\ldots,x_j}(y_k,\ldots,z_l)\), is defined to be: <sup><a href=#fn4>4</a></sup> $$\begin{equation} P^{w_i,\ldots,x_j}(y_k,\ldots,z_l) = \frac{P(w_i,\ldots,x_j,y_k,\ldots,z_l)}{P(w_i,\ldots,x_j)} \label{eq:1.2} \end{equation}$$ which represents the probability of the joint event \(Y = y_k,\ldots,Z = z_l\), conditioned by the fact that W,...,X are known to have taken the values \(w_i,\ldots,x_j\) respectively.</p>

    <p>For any numerical valued function \(F(y_k,\ldots,z_l)\), defined on the elements of the cartesian product of \(\cal Y,\ldots, Z\), the <em>expectation</em>, denoted by Exp [F], is defined to be: $$\begin{equation} \mathrm{Exp [F]} = \sum_{k\ldots l} P(y_k,\ldots,z_l)F(y_k,\ldots,z_l) \label{eq:1.3} \end{equation}$$</p>

    <p>We note that if \(P(y_k,\ldots,z_l)\) is a marginal distribution of some larger distribution \(P(w_i,\ldots,x_j,y_k,\ldots,z_l)\) then $$\begin{eqnarray} \mathrm{Exp[F]}  = & \sum_{k\ldots l}\big(\sum_{i\ldots j} P(w_i,\ldots,x_j,y_k,\ldots,z_l)\big) F(y_k,\ldots,z_l)\\ = & \sum_{i\ldots l} P(w_i,\ldots,x_j,y_k,\ldots,z_l) F(y_k,\ldots,z_l) \label{eq:1.4} \end{eqnarray}$$ so that if we wish to compute Exp [F] with respect to some joint distribution it suffices to use any marginal distribution of the original distribution which contains at least those variables which occur in F.</p>

    <p>We shall also occasionally be interested in <dfn>conditional expectations</dfn>, which we define as: $$\begin{equation} \mathrm{Exp^{w_i,\ldots,x_j}[F]} = \sum_{k\ldots l} P^{w_i,\ldots,x_j}(y_k,\ldots,z_l)F(y_k,\ldots,z_l) \label{eq:1.5} \end{equation}$$ and we note the following easily verified rules for expectations:
$$\begin{equation} \mathrm{Exp [Exp [F]] = Exp [F]} \label{eq:1.6} \end{equation}$$
$$\begin{equation} \mathrm{Exp^{u_i,\ldots,v_j}[Exp^{u_i,\ldots,v_j,w_k,\ldots,x_l}[F]]} = \mathrm{Exp^{u_i,\ldots,v_j}[F]} \label{eq:1.7}\end{equation}$$
$$\begin{equation} \mathrm{Exp[F+G]} = \mathrm{Exp[F]} + \mathrm{Exp[G]} \label{eq:1.8}\end{equation}$$
    </p>

    <p>We should like finally to comment upon the notion of <em>independence</em>.  Two random variables X and Y with join distribution \(P(x_i,y_j)\) will be said to be said to be independent if and only if \(P(x_i,y_j)\) is equal to \(P(x_i)P(y_j)\) for all i, j.  Similarly, the groups of random variables (U ... V), (W... X),..., (Y... Z) will be called mutually independent groups if and only if \(P(u_i,\ldots,v_j,w_k,\ldots,x_l,y_m,\ldots,z_n)\) is always equal to \(P(u_i,\ldots,v_j)P(w_k,\ldots,x_l)P(y_l,\ldots,z_n)\).</p>

    <p>Independence means that the random variables take on values which are not influenced by the values of other variables with respect to which they are independent. That is, the conditional distribution of one of two independent variables, Y, conditioned upon the value \(x_i\) for the other, is independent of \(x_i\), so that knowledge about one variable tells nothing of the other.</p>

    <h2>§2. Information for finite distributions</h2>

    <p>Suppose that we have a single random variable X, with distribution \(P(x_i)\).  We then define<sup><a href=#fn5>5</a></sup> a number, I<sub>X</sub>, called the <dfn>information</dfn> of X, to be: $$\begin{equation} I_X = \sum_i P(x_i) \ln P(x_i) = \mathrm{Exp}[\ln P(x_i)] \label{2.1} \end{equation}$$ which is a function of the probabilities alone and not of any possible numerical values of the x<sub>i</sub>'s themselves.<sup><a href=fn6>6</a></sup></p>

    <p>The information is essentially a measure of the sharpness of a probability distribution, that is, an inverse measure of its "spread." In this respect information plays a role similar to that of variance. However, it has a number of properties which make it a superior measure of the "sharpness" than the variance, not the least of which is the fact that it can be defined for distributions over arbitrary sets, while variance is defined only for distributions over real numbers.</p>

    <p>Any change in the distribution P(x<sub>i</sub>) which "levels out" the probabilities decreases the information.  It has the value zero for "perfectly sharp" distributions, in which the probability is one for one of the x<sub>i</sub> and zero for all others, and ranges downward to -ln n for distributions over n elements which are equal over all of the x<sub>i</sub>. The fact that the information is nonpositive is no liability, since we are seldom interested in the absolute information of a distribution, but only in differences.</p>

    <p>We can generalize (\ref{2.1}) to obtain the formula for the information of a <em>group</em> of random variables X, V, ..., Z, with joint distribution \(P(x_i,y_j,\ldots ,z_k)\) which we denote by I<sub>XV ... Z</sub>: $$\begin{eqnarray} I_{XY\ldots Y} = & \sum_{i,j,\ldots,k} P(x_i, y_j,\ldots, z_k) \ln P(x_i, y_j,\ldots, z_k)\\ = & \mathrm{Exp}[\ln P(x_i, y_j,\ldots, z_k)] \label{2.2} \end{eqnarray}$$ which follows immediately from our previous definition, since the group of random variables X, Y, ... ,Z may be regarded as a single random variable W which takes its values in the cartesian product \(\cal X \times Y \times \ldots \times Z\).  </p>

    <h1>Notes</h1>
    <ol>
    <li id=fn1>We use the terminology of von Neumann <a href="#b17">[17]</a></li>
    <li id=fn2>In the words of von Neumann (<a href="#b17">[17]</a>, p. 418):
        <blockquote> ..... it is a fundamental requirement of the scientific viewpoint - the so-called principle of the psycho-physical parallelism - that it must be possible so to describe the extra-physical process of the subjective perception as if it were in reality in the physical world - i.e., to as'sign to its parts equivalent physical processes in the objective environment, in ordinary space.</blockquote>
    </li>
    <li id=fn3>The theory originated by Claude E. Shannon <a href="#b19">[19]</a></li>
    <li id=fn4>We regard it as undefined if \(P(w_i,\ldots,x_j) = 0\).  In this case \(P(w_i,\ldots,x_j,y_k,\ldots,z_l)\) is necessarily also zero.</li>
    <li id=fn5>This definition corresponds to the negative of the <em>entropy</em> of a probability distribution as defined by Shannon <a href="#b19">[19]</a></li>
    </ol>

    <h1>References</h1>
    <ol>
    <li id="b17">J. von Neumann, <cite>Mathematical Foundations of Quantum Mechanics</cite>.  (Translated by R. T. Beyer) Princeton University Press: 1955.</li>
    <li id="b19">C. E. Shannon, W. Weaver, <cite>The Mathematical Theory of Communication</cite>.  University of Illinois Press: 1949.</li>
    </ol>

    </body>
</html>
